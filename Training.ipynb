{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path way setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "id": "EcFySnMg6QK0",
    "outputId": "2319b451-1a60-4478-8e69-f92f093b9e9f"
   },
   "outputs": [],
   "source": [
    "# If storage the dataset in google dirve run this cell and the cell below\n",
    "# set the environment for google drive\n",
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUddxxFC61I-"
   },
   "outputs": [],
   "source": [
    "# mark the direction to Google Drive\n",
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlodCdOOD9mj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as image\n",
    "from os.path import join\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bx-VqczND9ms"
   },
   "outputs": [],
   "source": [
    "# if the device has a GPU, run the programm with GPU, otherwise run with CPU\n",
    "device = \"gpu:0\" if tfe.num_gpus() else \"cpu:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7XDFS6W5D9mu"
   },
   "outputs": [],
   "source": [
    "# load decorded dataset \n",
    "npz_path = 'drive/Resized_Histo_50_20000.npz'  # set the path way of data\n",
    "data = np.load(npz_path)\n",
    "\n",
    "X = data['image']   # select the 'image' column in data to X\n",
    "train_labels = data['labels']   # select the 'labels' column in data to train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6f0ht_AXD9nm"
   },
   "outputs": [],
   "source": [
    "# split dataset into train, test and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, train_labels, test_size=0.1)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6xdcKFeBD9np"
   },
   "outputs": [],
   "source": [
    "# build the network with pre-trained Xception model\n",
    "class Classifier(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.xception_layers = Xception(include_top=False, weights='imagenet')  \n",
    "        self.pooling_layer = GlobalAveragePooling2D()\n",
    "        self.dense_layer1 = tf.keras.layers.Dense(units=128, activation='relu')\n",
    "        self.dense_layer2 = tf.keras.layers.Dense(units=64, activation='relu')\n",
    "        self.dropout_layer1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.dense_layer3 = tf.keras.layers.Dense(units=n_classes)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        result = self.xception_layers(inputs)\n",
    "        result = self.pooling_layer(result)\n",
    "        result = self.dense_layer1(result)\n",
    "        result = self.dense_layer2(result)\n",
    "        result = self.dropout_layer1(result)\n",
    "        result = self.dense_layer3(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0pMqums53Iz"
   },
   "outputs": [],
   "source": [
    "# build the network with pre-trained Inception model\n",
    "class Classifier(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.xception_layers = InceptionV3(include_top=False, weights='imagenet')  \n",
    "        self.pooling_layer = GlobalAveragePooling2D()\n",
    "        self.dense_layer1 = tf.keras.layers.Dense(units=20, activation='relu')\n",
    "        self.dense_layer2 = tf.keras.layers.Dense(units=20, activation='relu')\n",
    "        self.dense_layer3 = tf.keras.layers.Dense(units=n_classes)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        result = self.xception_layers(inputs)\n",
    "        result = self.pooling_layer(result)\n",
    "        result = self.dense_layer1(result)\n",
    "        result = self.dense_layer2(result)\n",
    "        result = self.dense_layer3(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oFkysU55D9ns"
   },
   "outputs": [],
   "source": [
    "# build shallow convolution neural network\n",
    "class Classifier(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.convolution_layer1 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu')\n",
    "        self.convolution_layer = tf.keras.layers.Conv2D(96, (3, 3), activation='relu')\n",
    "        self.pooling_layer = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n",
    "        self.dropout_layer1 = tf.keras.layers.Dropout(0.25)\n",
    "        self.flatten_layer = tf.keras.layers.Flatten()\n",
    "        self.dense_layer1 = tf.keras.layers.Dense(units=128, activation='relu')\n",
    "        self.dropout_layer2 = tf.keras.layers.Dropout(0.5)  \n",
    "        self.dense_layer2 = tf.keras.layers.Dense(units=n_classes)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        result = self.convolution_layer(inputs)\n",
    "        result = self.pooling_layer(result)\n",
    "        result = self.dropout_layer1(result)\n",
    "        result = self.flatten_layer(result)\n",
    "        result = self.dense_layer1(result)\n",
    "        result = self.dropout_layer2(result)\n",
    "        result = self.dense_layer2(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqUGc4DiD9nv"
   },
   "outputs": [],
   "source": [
    "# parameters setting\n",
    "batch_size = 100\n",
    "n_epochs = 50\n",
    "learning_rate = 0.0001\n",
    "\n",
    "train_loss_history = []\n",
    "validation_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjyKTKc3D9ny"
   },
   "outputs": [],
   "source": [
    "# slice the dataset and shuffle it\n",
    "\n",
    "train_images_dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "train_labels_dataset = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "train_dataset = tf.data.Dataset.zip((train_images_dataset, train_labels_dataset))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(X_train)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eLwnnlEYD9n1"
   },
   "outputs": [],
   "source": [
    "# initiate the model and set the model with optimizer\n",
    "\n",
    "x_classifier = Classifier(n_classes=2)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1994
    },
    "colab_type": "code",
    "id": "l2j_xQQBD9n5",
    "outputId": "87d51752-b399-423c-a93a-f951f61311c4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training process\n",
    "\n",
    "with tf.device(device):\n",
    "    losscount = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        start = time.time()\n",
    "        for batch, (images, labels) in enumerate(train_dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Compute logits\n",
    "                logits = x_classifier(images)\n",
    "                \n",
    "                # Coumpute loss\n",
    "                xe_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))                \n",
    "                train_loss_history.append(xe_loss.numpy())\n",
    "            # Compute gradient and apply gradients\n",
    "                                         \n",
    "            grads = tape.gradient(xe_loss, x_classifier.variables)\n",
    "            optimizer.apply_gradients(zip(grads, x_classifier.variables),global_step=tf.train.get_or_create_global_step())\n",
    "            \n",
    "            \n",
    "            if batch % 10 == 0:\n",
    "                print('\\rEpoch: {}, Batch: {}, Loss: {}'.format(epoch, batch, train_loss_history[-1]), end='')\n",
    "\n",
    "        # Compute validation set    \n",
    "        logits_v = x_classifier(tf.constant(X_validation))\n",
    "        xe_loss_v = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_validation, logits=logits_v))\n",
    "        validation_loss_history.append(xe_loss_v.numpy()) \n",
    "         \n",
    "        # early stop    \n",
    "        if len(validation_loss_history) >= 5:\n",
    "            if xe_loss_v > np.array(validation_loss_history[-1:-6:-1]).mean():\n",
    "                losscount +=1\n",
    "            else:\n",
    "                losscount = 0\n",
    "                \n",
    "        if losscount > 3:\n",
    "            break\n",
    "       \n",
    "        print('\\rEpoch: {}, Batch: {}, Train Loss: {}, Count: {}'.format(epoch, batch, train_loss_history[-1],losscount))\n",
    "        print('\\rEpoch: {}, Batch: {}, Validation Loss: {}, Count: {}'.format(epoch, batch, validation_loss_history[-1],losscount))\n",
    "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "5JeGHZplD9oD",
    "outputId": "7ebd5831-f5c7-47bb-c1c7-20d633b42edb"
   },
   "outputs": [],
   "source": [
    "# make plot for train loss\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(train_loss_history)\n",
    "plt.xlabel('Iterations', fontsize=14)\n",
    "plt.ylabel('loss', fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "IKZaj4HfD9n_",
    "outputId": "09d6d5fa-7b14-4838-d4fa-0b16dcb347d8"
   },
   "outputs": [],
   "source": [
    "# make plot for validation loss\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(validation_loss_history)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('loss', fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b1J4ddZRD9oP"
   },
   "outputs": [],
   "source": [
    "# apply softmax to calculatehe probabilities for each classes\n",
    "logits = x_classifier(tf.constant(X_test))\n",
    "y_pred = tf.nn.softmax(logits).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3EvdC07hD9oR",
    "outputId": "7d2e8fe5-93f7-4fe4-9598-70c3465659bc"
   },
   "outputs": [],
   "source": [
    "# calculate accuracy\n",
    "accuracy_result_1=[]\n",
    "for i in range(len(y_test)):\n",
    "    accuracy_result_1.append(y_test[i] - np.argmax(y_pred,axis=1)[i])\n",
    "accuracy_result_1.count(0)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3FxZ_AnMFyAE"
   },
   "outputs": [],
   "source": [
    "# predict rsult \n",
    "y_pred_result =  np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jxfQ40kkFYY6"
   },
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "    normalize=False,\n",
    "    title='Confusion matrix',\n",
    "    cmap=plt.cm.Blues):\n",
    "    import itertools\n",
    "    \"\"\"\n",
    "     This function prints and plots the confusion matrix.\n",
    "     Normalization can be applied by setting `normalize=True`.\n",
    "     \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    #plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),horizontalalignment=\"center\",color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Diagnosis')\n",
    "    plt.xlabel('Predicted Diagnosis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "Q-8rLrgTFokr",
    "outputId": "d0979cf9-73f6-4416-c377-6a820ce49f34"
   },
   "outputs": [],
   "source": [
    "# plot for test set\n",
    "cm=confusion_matrix(y_test, y_pred_result,np.unique(y_test))\n",
    "plot_confusion_matrix(cm,np.unique(y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "xLT_fdjoHNDj",
    "outputId": "3954dcde-397d-49c4-937d-c1b74582e123"
   },
   "outputs": [],
   "source": [
    "# make ROC plot and calcualte AUC\n",
    "import sklearn.metrics as metrics\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred[:,1])\n",
    "#fpr_n, tpr_n, threshold_n = metrics.roc_curve(y_test, y_pred[:,1])\n",
    "\n",
    "# Calculate AUC area and print\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "print('Auc value based on Xception:', roc_auc)\n",
    "\n",
    "# Plot ROC curve for test dataset\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'Xception, AUC = %0.2f' % roc_auc)\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Histopathology 75.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
