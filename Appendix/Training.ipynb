{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = \"./Cache/\"\n",
    "fname = 'bottle_neck.npz'\n",
    "save_path = os.path.join(cache_path,fname)\n",
    "\n",
    "device = \"gpu:0\" if tfe.num_gpus() else \"cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XceptionClassifier(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super(XceptionClassifier, self).__init__()\n",
    "        # Define the layer(s) you would like to use for your classifier\n",
    "    # 5 layers\n",
    "#         self.dense_layer1 = tf.keras.layers.Dense(units=4096, activation='relu')\n",
    "#         self.dense_layer2 = tf.keras.layers.Dense(units=2048, activation='relu')\n",
    "#         self.dense_layer3 = tf.keras.layers.Dense(units=1024, activation='relu')\n",
    "#         self.dense_layer4 = tf.keras.layers.Dense(units=256, activation='relu')\n",
    "#         self.dense_layer5 = tf.keras.layers.Dense(units=n_classes)\n",
    "#         # ... additional layers if you want...\n",
    "    # 3 layers\n",
    "#         self.dense_layer1 = tf.keras.layers.Dense(units=20, activation='relu')\n",
    "#         self.dense_layer2 = tf.keras.layers.Dense(units=20, activation='relu')\n",
    "#         self.dense_layer3 = tf.keras.layers.Dense(units=20, activation='relu')\n",
    "#         self.dense_layer4 = tf.keras.layers.Dense(units=n_classes)\n",
    "        \n",
    "    # 2 layers\n",
    "        self.dense_layer1 = tf.keras.layers.Dense(units=1024, activation='relu')\n",
    "        self.dense_layer2 = tf.keras.layers.Dense(units=n_classes)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Set this up appropriately, will depend on how many layers you choose\n",
    "#         result = self.dense_layer1(inputs)\n",
    "#         result = self.dense_layer2(result)\n",
    "#         result = self.dense_layer3(result)\n",
    "#         result = self.dense_layer4(result)\n",
    "#         result = self.dense_layer5(result)\n",
    "        \n",
    "        # ... if additional layers...\n",
    "#         result = self.dense_layer1(inputs)\n",
    "#         result = self.dense_layer2(result)\n",
    "#         result = self.dense_layer3(result)\n",
    "#         result = self.dense_layer4(result)\n",
    "\n",
    "        result = self.dense_layer1(inputs)\n",
    "        result = self.dense_layer2(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(save_path)\n",
    "train_bottle_necks, train_labels = data['bottle_necks'],  data['labels']\n",
    "n_train_samples= len(train_bottle_necks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization if necessary\n",
    "for i in range(n_train_samples):\n",
    "    train_bottle_necks[i] = np.true_divide(train_bottle_necks[i], max(train_bottle_necks[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, X_test, Y, Y_test = train_test_split(train_bottle_necks,train_labels, test_size=0.2)\n",
    "X_train, X_v, Y_train, Y_v = train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20        # You will play around with this \n",
    "n_epochs = 200         # Choose num epochs based on when you think the parameters have converged\n",
    "learning_rate = 0.001 # You will try different learning rates\n",
    "\n",
    "train_loss_history = []\n",
    "validation_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "train_labels_dataset = tf.data.Dataset.from_tensor_slices(Y_train)\n",
    "train_dataset = tf.data.Dataset.zip((train_images_dataset, train_labels_dataset))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(X_train)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_classifier = XceptionClassifier(n_classes=2)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "    losscount = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch, (images, labels) in enumerate(train_dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Compute logits\n",
    "                logits = x_classifier(images)\n",
    "                xe_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))                \n",
    "                train_loss_history.append(xe_loss.numpy())\n",
    "            # Compute gradient and apply gradients\n",
    "                                         \n",
    "            grads = tape.gradient(xe_loss, x_classifier.variables)\n",
    "            optimizer.apply_gradients(zip(grads, x_classifier.variables),global_step=tf.train.get_or_create_global_step())\n",
    "            \n",
    "            \n",
    "            if batch % 10 == 0:\n",
    "                print('\\rEpoch: {}, Batch: {}, Loss: {}'.format(epoch, batch, train_loss_history[-1]), end='')\n",
    "                #print('\\rEpoch: {}, Batch: {}, Loss(validation): {}'.format(epoch, batch, validation_loss_history[-1]), end='')\n",
    "                                         \n",
    "        #for batch, (images,labels) in enumerate(validation_dataset):\n",
    "        logits_v = x_classifier(X_v)\n",
    "        xe_loss_v = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y_v, logits=logits_v))\n",
    "        \n",
    "        \n",
    "        if len(validation_loss_history) >= 5:\n",
    "            if xe_loss_v > np.array(validation_loss_history[-1:-3:-1]).mean():\n",
    "                losscount +=1\n",
    "            else:\n",
    "                losscount = 0\n",
    "                \n",
    "        if losscount > 2:\n",
    "            break\n",
    "        validation_loss_history.append(xe_loss_v.numpy()) \n",
    "       \n",
    "        print('\\rEpoch: {}, Batch: {}, Train Loss: {},Count: {}'.format(epoch, batch, train_loss_history[-1],losscount))\n",
    "        print('\\rEpoch: {}, Batch: {}, Validation Loss: {},Count: {}'.format(epoch, batch, validation_loss_history[-1],losscount))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(validation_loss_history)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('loss', fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(train_loss_history)\n",
    "plt.xlabel('Iterations', fontsize=14)\n",
    "plt.ylabel('loss', fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = x_classifier(tf.constant(X_test))\n",
    "y_pred = tf.nn.softmax(logits).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_result_1=[]\n",
    "for i in range(len(Y_test)):\n",
    "    accuracy_result_1.append(Y_test[i] - np.argmax(y_pred,axis=1)[i])\n",
    "accuracy_result_1.count(0)/len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = x_classifier(tf.constant(X_train))\n",
    "y_pred_train = tf.nn.softmax(logits).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_result_2=[]\n",
    "for i in range(len(Y_train)):\n",
    "    accuracy_result_2.append(Y_train[i] - np.argmax(y_pred_train,axis=1)[i])\n",
    "accuracy_result_2.count(0)/len(Y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
